Times:

10 simulations: 0m0.022s
100 simulations: 0m0.042s
1000 simulations: 0m0.029s
10000 simulations: 0m0.032s
100000 simulations: 0m0.087s
1000000 simulations: 0m0.652s

Questions:

Which predictions, if any, proved incorrect as you increased the number of simulations?

With a small number of simulations, it would prove to vbe wrong.

Suppose you're charged a fee for each second of compute time your program uses.
After how many simulations would you call the predictions "good enough"?

It seems like the predictions stabilized after about 10000 simulation.